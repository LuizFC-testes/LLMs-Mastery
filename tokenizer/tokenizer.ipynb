{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86a242b2d0d4246bb885b7c05d56520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6a14946a0e4be5b2c7f3819fa49c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79702b7c98de44ec952e11d78018ff0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb770e10fba4454f9dc7a8da5237160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8ceb5e2cd644b4b7e441c7cf3753d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_t = \"Uma dÃ¡diva dos ninjas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Um', '##a', 'd', '##Ã¡', '##di', '##va', 'dos', 'ni', '##nja', '##s']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_t = tokenizer.tokenize(sentence_t)\n",
    "tokens_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The less we stare a different life, I play the role, I smile bright, consume the perfect pesticide and laugh at this so childlike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'less',\n",
       " 'we',\n",
       " 'stare',\n",
       " 'a',\n",
       " 'different',\n",
       " 'life',\n",
       " ',',\n",
       " 'I',\n",
       " 'play',\n",
       " 'the',\n",
       " 'role',\n",
       " ',',\n",
       " 'I',\n",
       " 'smile',\n",
       " 'bright',\n",
       " ',',\n",
       " 'consume',\n",
       " 'the',\n",
       " 'perfect',\n",
       " 'p',\n",
       " '##est',\n",
       " '##icide',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'this',\n",
       " 'so',\n",
       " 'child',\n",
       " '##like']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1109,\n",
       " 1750,\n",
       " 1195,\n",
       " 5698,\n",
       " 170,\n",
       " 1472,\n",
       " 1297,\n",
       " 117,\n",
       " 146,\n",
       " 1505,\n",
       " 1103,\n",
       " 1648,\n",
       " 117,\n",
       " 146,\n",
       " 2003,\n",
       " 3999,\n",
       " 117,\n",
       " 17914,\n",
       " 1103,\n",
       " 3264,\n",
       " 185,\n",
       " 2556,\n",
       " 24421,\n",
       " 1105,\n",
       " 4046,\n",
       " 1120,\n",
       " 1142,\n",
       " 1177,\n",
       " 2027,\n",
       " 10318,\n",
       " 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(sentence)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1109, 1750, 1195, 5698, 170, 1472, 1297, 117, 146, 1505, 1103, 1648, 117, 146, 2003, 3999, 117, 17914, 1103, 3264, 185, 2556, 24421, 1105, 4046, 1120, 1142, 1177, 2027, 10318, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_out = tokenizer(sentence)\n",
    "tokenizer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1109,  1750,  1195,  5698,   170,  1472,  1297,   117,   146,\n",
       "          1505,  1103,  1648,   117,   146,  2003,  3999,   117, 17914,  1103,\n",
       "          3264,   185,  2556, 24421,  1105,  4046,  1120,  1142,  1177,  2027,\n",
       "         10318,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2572,  0.2389, -0.0298,  ..., -0.2512,  0.2855, -0.1198],\n",
       "         [ 0.0433, -0.8083,  0.6628,  ..., -0.1886,  0.3184, -0.0452],\n",
       "         [ 0.4463, -0.3743, -0.2149,  ...,  0.1187,  0.9708, -0.5822],\n",
       "         ...,\n",
       "         [ 0.0879,  0.1421,  0.5430,  ...,  0.2689,  0.1080,  0.4416],\n",
       "         [ 0.1649, -0.0150, -0.1331,  ..., -0.8033,  0.3669, -0.0743],\n",
       "         [ 1.1507, -0.0037, -0.0066,  ..., -0.3854,  0.6845, -0.2912]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-5.5841e-01,  3.9147e-01,  9.9962e-01, -9.9113e-01,  9.1176e-01,\n",
       "          9.6566e-01,  9.7960e-01, -9.9749e-01, -9.3786e-01, -4.1796e-01,\n",
       "          9.7021e-01,  9.9811e-01, -9.9947e-01, -9.9939e-01,  8.8690e-01,\n",
       "         -9.7262e-01,  9.8199e-01, -4.8371e-01, -9.9990e-01, -8.8366e-01,\n",
       "         -6.3485e-01, -9.9969e-01,  1.5352e-01,  9.8619e-01,  9.6885e-01,\n",
       "          3.8029e-02,  9.8348e-01,  9.9992e-01,  7.7503e-01, -1.9435e-01,\n",
       "          1.8745e-01, -9.8551e-01,  8.5552e-01, -9.9733e-01,  1.0726e-01,\n",
       "          3.4879e-01,  6.1582e-01, -1.5127e-01,  8.8545e-01, -9.6238e-01,\n",
       "         -3.2001e-01, -8.6279e-01,  8.0284e-01, -4.7852e-01,  9.2805e-01,\n",
       "          1.7517e-01, -7.5959e-02, -1.3194e-03,  7.9265e-02,  9.9878e-01,\n",
       "         -9.3329e-01,  9.1208e-01, -9.9796e-01,  9.7702e-01,  9.9400e-01,\n",
       "          2.0813e-01,  9.9303e-01,  9.6318e-02, -9.9966e-01, -8.8637e-02,\n",
       "          9.3443e-01,  2.6158e-01,  8.5131e-01,  7.4916e-02,  3.9414e-01,\n",
       "         -3.1669e-01, -8.8949e-01,  1.0528e-01, -3.0754e-01,  2.8271e-01,\n",
       "          7.2206e-02,  3.2506e-01,  9.7605e-01, -8.4211e-01,  9.4257e-04,\n",
       "         -8.4200e-01, -7.9784e-02, -9.9964e-01,  8.8408e-01,  9.9989e-01,\n",
       "          8.6888e-01, -9.9956e-01,  9.9304e-01, -7.6100e-02, -8.1088e-01,\n",
       "          7.4931e-01, -9.9955e-01, -9.9885e-01,  7.0982e-02, -3.6686e-01,\n",
       "          9.8047e-01, -9.8240e-01,  7.4825e-01, -9.5068e-01,  9.9993e-01,\n",
       "         -9.3551e-01, -7.5499e-02,  3.2698e-01,  9.6751e-01, -8.8743e-01,\n",
       "         -5.9144e-01,  9.6535e-01,  9.9965e-01, -9.9818e-01,  9.9929e-01,\n",
       "          7.9819e-01, -9.0887e-01, -9.5634e-01,  8.1792e-01,  1.7379e-02,\n",
       "          9.8425e-01, -9.7946e-01, -8.9677e-01,  1.5035e-04,  9.6081e-01,\n",
       "         -9.3574e-01,  9.8402e-01,  8.6188e-01, -2.4450e-01,  9.9994e-01,\n",
       "         -5.8155e-02,  9.6010e-01,  9.9764e-01,  8.7602e-01, -9.0737e-01,\n",
       "         -8.4130e-02, -7.2497e-01,  9.2877e-01, -6.3478e-01,  2.5402e-02,\n",
       "          6.8193e-01, -9.8560e-01, -9.9894e-01,  9.9900e-01, -1.9864e-01,\n",
       "          9.9991e-01, -9.9840e-01,  9.9752e-01, -9.9982e-01, -9.1490e-01,\n",
       "         -8.7782e-01,  3.5617e-02, -9.9141e-01,  5.7968e-02,  9.8335e-01,\n",
       "         -8.6880e-02, -9.5471e-01, -9.5465e-01,  7.4248e-01, -9.2729e-01,\n",
       "          3.7484e-01,  7.0978e-01, -9.4269e-01,  8.2880e-01,  9.9892e-01,\n",
       "          9.7364e-01,  9.9078e-01,  1.1600e-01, -9.7098e-01,  9.6317e-01,\n",
       "          9.7641e-01, -9.9919e-01,  7.0852e-01, -9.9776e-01,  9.9769e-01,\n",
       "          9.3818e-01,  8.9992e-01, -9.9885e-01,  9.9979e-01, -6.3975e-01,\n",
       "         -2.5238e-01, -2.4215e-01,  3.3145e-02, -9.9958e-01,  3.6204e-01,\n",
       "          3.6226e-01,  5.2469e-01,  9.9858e-01, -9.9574e-01,  9.9816e-01,\n",
       "          3.3606e-01,  5.1863e-01,  6.0619e-01,  9.9969e-01, -9.9264e-01,\n",
       "         -9.7609e-01, -9.7751e-01,  1.4104e-01,  9.0023e-01,  7.7297e-01,\n",
       "          5.6734e-01,  9.5641e-01,  9.9944e-01,  4.3839e-01, -9.9400e-01,\n",
       "         -3.6439e-01,  9.6955e-01, -1.3928e-01,  9.9992e-01, -1.4216e-01,\n",
       "         -9.9955e-01, -7.6265e-01,  9.5886e-01,  9.8934e-01, -1.5149e-01,\n",
       "          9.7346e-01, -8.4147e-01, -4.8670e-01,  9.9421e-01, -8.0832e-01,\n",
       "          9.9940e-01,  1.6207e-01,  9.2038e-01,  7.8660e-01,  9.9035e-01,\n",
       "         -9.1756e-01,  3.6983e-02,  1.3609e-01, -8.2692e-01,  9.9971e-01,\n",
       "         -9.9915e-01, -1.5924e-01,  1.3701e-01, -9.8915e-01, -9.9698e-01,\n",
       "          9.6728e-01,  1.7631e-01, -7.6791e-01, -1.1298e-01,  8.5211e-01,\n",
       "          2.1378e-01,  9.4982e-01,  9.8093e-01, -7.5022e-01, -8.4363e-01,\n",
       "         -9.9958e-01, -9.9881e-01, -5.7489e-01, -9.8629e-01,  5.3743e-03,\n",
       "          5.7623e-01, -2.7467e-01, -8.2116e-01, -9.9931e-01,  9.5526e-01,\n",
       "          9.2361e-01, -9.5380e-01,  2.0344e-02, -8.0600e-01, -9.9926e-01,\n",
       "          7.5081e-01, -9.3192e-01, -9.9669e-01,  9.9899e-01, -9.0151e-01,\n",
       "          9.9835e-01,  9.6266e-01, -9.9038e-01,  8.4175e-01, -9.9963e-01,\n",
       "         -1.0028e-02, -9.0005e-01,  4.8727e-01,  7.6298e-01, -8.7092e-01,\n",
       "          9.6353e-02,  9.9068e-01, -9.3602e-01, -7.9040e-01,  8.9295e-01,\n",
       "         -9.9979e-01,  8.6998e-01, -6.3950e-02,  9.9853e-01,  8.9635e-01,\n",
       "          4.2796e-02,  9.7184e-01,  9.6444e-01, -9.8363e-01, -9.9948e-01,\n",
       "          9.6670e-01,  5.3328e-01, -9.8904e-01, -1.1838e-01,  9.9985e-01,\n",
       "         -9.9916e-01, -6.1479e-01, -9.3399e-01, -9.8961e-01, -9.9924e-01,\n",
       "          2.8678e-01, -8.8598e-01,  2.5427e-01,  9.7712e-01,  5.1239e-01,\n",
       "          2.7457e-01,  9.9251e-01,  9.4971e-01,  3.7206e-01, -3.6551e-01,\n",
       "         -4.7197e-02, -9.5066e-01, -6.3125e-01,  7.0981e-01,  1.5980e-01,\n",
       "         -9.9991e-01,  9.9962e-01, -9.9122e-01,  8.8288e-01,  9.4685e-01,\n",
       "         -9.9870e-01,  6.8692e-01,  1.6871e-01, -9.8116e-01, -2.2300e-03,\n",
       "          9.9982e-01,  9.8147e-01,  6.2833e-02,  5.2003e-02,  9.1641e-01,\n",
       "         -3.9534e-01,  7.8414e-01, -8.9166e-01, -7.5761e-01,  1.7905e-02,\n",
       "         -8.9752e-01,  9.9658e-01,  8.8642e-01, -9.8595e-01,  9.9868e-01,\n",
       "          1.2155e-02,  5.1033e-01, -9.2990e-01,  7.7952e-01,  9.8413e-01,\n",
       "         -3.6262e-02, -6.5827e-01, -1.7524e-01,  3.6383e-01, -9.9344e-01,\n",
       "         -1.0242e-01, -9.9932e-01, -4.9325e-01,  8.3854e-01,  9.8486e-01,\n",
       "         -9.9078e-01,  9.5964e-01, -8.4110e-02,  9.5833e-01, -9.9918e-01,\n",
       "          9.9994e-01, -9.9327e-01,  1.7138e-02,  8.4315e-01, -8.1022e-01,\n",
       "         -6.8445e-01,  9.8669e-01,  9.9387e-01,  9.9074e-01, -6.0825e-01,\n",
       "         -7.3058e-01,  6.1487e-01,  9.6146e-01, -9.8107e-01,  4.2401e-02,\n",
       "         -9.9975e-01, -8.4134e-01,  9.9378e-01,  9.9881e-01, -3.7696e-02,\n",
       "         -3.7878e-01, -9.9928e-01,  9.5096e-01, -8.6956e-01, -9.7437e-01,\n",
       "          3.5019e-02, -9.0675e-01,  9.2109e-01,  9.9906e-01, -7.7434e-01,\n",
       "          8.3122e-01,  8.6834e-02, -9.7944e-01,  9.4941e-01,  8.9709e-01,\n",
       "          9.9968e-01, -9.7070e-01,  6.8928e-01,  9.8207e-01, -8.8133e-02,\n",
       "         -8.3613e-01,  4.4580e-01,  9.9970e-01, -9.5935e-01, -1.8833e-01,\n",
       "         -9.9925e-01,  1.9124e-02, -7.5620e-01, -1.6807e-02, -6.7147e-01,\n",
       "          1.1704e-02, -9.1709e-01,  9.8595e-01,  3.2699e-01,  6.7708e-01,\n",
       "         -5.1378e-01,  9.3862e-01, -4.9109e-01,  3.0004e-02, -2.4330e-01,\n",
       "         -1.5150e-01,  4.6587e-01,  3.7403e-01,  9.7512e-01, -9.6028e-01,\n",
       "          9.9913e-01, -1.3818e-03, -9.9990e-01, -9.9898e-01, -9.1645e-01,\n",
       "         -9.9782e-01,  7.5073e-01, -9.7469e-01,  9.8564e-01,  9.4596e-01,\n",
       "         -9.9948e-01, -9.9967e-01, -9.8809e-01, -9.2016e-01,  9.7301e-01,\n",
       "          7.7937e-01,  1.3853e-01, -1.7451e-02, -6.4495e-01,  3.2993e-02,\n",
       "         -4.8784e-01, -4.5812e-02, -9.0130e-01, -7.1666e-01, -9.9925e-01,\n",
       "          8.1854e-01, -9.9990e-01, -8.4949e-01,  9.9880e-01, -9.9842e-01,\n",
       "         -9.0919e-01, -9.0662e-01, -9.4180e-01, -7.0903e-01,  1.8278e-01,\n",
       "          9.7483e-01, -1.4345e-01, -6.7182e-01, -9.9827e-01,  9.7980e-01,\n",
       "         -7.2450e-01,  6.1914e-02, -9.1137e-01, -9.4770e-01,  9.9923e-01,\n",
       "          8.8999e-01, -2.3031e-01,  8.4269e-02, -9.9754e-01,  9.9320e-01,\n",
       "         -9.6506e-01, -9.5351e-01, -9.7521e-01,  3.6505e-03, -8.7721e-01,\n",
       "         -9.9950e-01, -1.0408e-01,  9.9823e-01,  9.7741e-01,  9.6552e-01,\n",
       "         -9.7228e-02, -2.8382e-01, -9.2743e-01, -1.0208e-02, -9.9985e-01,\n",
       "          8.7882e-01,  9.3434e-01, -9.5758e-01, -8.8396e-01,  9.8579e-01,\n",
       "          9.7149e-01, -9.7479e-01, -9.8436e-01,  9.6935e-01,  3.2989e-01,\n",
       "          9.4027e-01, -7.9765e-01, -2.9341e-01,  1.8188e-01,  5.6008e-02,\n",
       "         -9.8585e-01, -9.1105e-01,  9.9478e-01, -9.9968e-01,  9.7652e-01,\n",
       "          9.9709e-01,  9.9950e-01, -2.6958e-01,  2.4154e-01, -9.9166e-01,\n",
       "         -8.5192e-01, -6.0679e-01,  4.9048e-01, -9.9982e-01,  9.9968e-01,\n",
       "         -9.9990e-01,  7.5383e-01, -8.4539e-01,  8.9650e-01,  9.8941e-01,\n",
       "         -4.5382e-01, -9.9976e-01, -9.9959e-01,  3.0457e-01,  1.0091e-01,\n",
       "          9.8609e-01,  2.9928e-01,  7.6523e-03, -7.7903e-01, -6.1316e-01,\n",
       "          9.9589e-01, -5.4006e-01, -8.0994e-01, -9.9918e-01,  9.9941e-01,\n",
       "          6.0625e-01, -9.9790e-01,  9.9861e-01, -9.9868e-01,  9.3763e-01,\n",
       "          9.6262e-01,  8.3367e-01,  9.5135e-01, -9.9961e-01,  9.9989e-01,\n",
       "         -9.9957e-01,  9.8868e-01, -9.9992e-01, -9.9971e-01,  9.9961e-01,\n",
       "         -9.8199e-01, -9.3653e-01, -9.9933e-01, -9.9933e-01,  8.2362e-01,\n",
       "          6.3313e-02, -2.8523e-01,  9.8670e-01, -9.9960e-01, -9.9821e-01,\n",
       "          3.2153e-01, -9.4472e-01, -9.0652e-01,  9.9800e-01, -6.9677e-01,\n",
       "          9.8869e-01, -3.0009e-01,  9.2949e-01,  3.0180e-01,  9.9906e-01,\n",
       "          9.3942e-01, -9.1090e-01, -4.9029e-01, -9.9116e-01,  9.7506e-01,\n",
       "         -8.0078e-01,  3.1254e-01,  9.5194e-01,  1.9037e-01, -7.8950e-01,\n",
       "          4.1905e-01, -9.9597e-01,  5.5920e-01,  3.5063e-01,  9.5576e-01,\n",
       "          9.5030e-01,  7.6181e-01, -1.3880e-01, -6.8541e-01, -3.7730e-01,\n",
       "         -9.8965e-01,  4.9191e-01, -9.9919e-01,  9.6805e-01, -9.6597e-01,\n",
       "         -1.0296e-01, -3.9286e-01,  2.5082e-01, -9.5586e-01,  9.9908e-01,\n",
       "          9.9671e-01, -8.8682e-01,  1.5336e-03,  9.8733e-01, -8.1910e-01,\n",
       "          9.6841e-01, -9.8808e-01,  7.1206e-02,  9.8448e-01, -6.5667e-01,\n",
       "          9.7570e-01,  5.5076e-02,  2.6980e-04,  9.7263e-01, -9.9272e-01,\n",
       "         -9.5754e-01, -5.8538e-01,  5.0370e-01, -2.2019e-01, -9.6129e-01,\n",
       "          4.0070e-03,  9.9245e-01, -4.1624e-01, -9.9955e-01,  9.3949e-01,\n",
       "         -9.9887e-01,  1.0252e-02,  9.6896e-01, -4.9668e-01,  9.9976e-01,\n",
       "         -9.0038e-01,  2.1705e-01,  2.4183e-01, -9.9962e-01, -9.9973e-01,\n",
       "         -6.9678e-02, -1.0803e-01, -9.5664e-01,  9.9973e-01,  4.0453e-03,\n",
       "          8.9111e-01, -9.9952e-01,  2.3674e-01,  9.9851e-01,  1.9252e-01,\n",
       "          5.5326e-01, -8.8459e-01, -9.2747e-01, -9.8140e-01, -4.3234e-01,\n",
       "         -1.5972e-01,  9.6868e-01, -9.8308e-01, -8.4194e-01, -9.3397e-01,\n",
       "          9.9985e-01, -9.9678e-01, -9.2179e-01, -9.8325e-01,  5.3242e-01,\n",
       "          8.7474e-01,  4.0220e-01, -4.8328e-03, -9.6557e-01,  9.6634e-01,\n",
       "         -9.4088e-01,  9.9465e-01, -9.9373e-01, -9.9470e-01,  9.9958e-01,\n",
       "          4.2113e-01, -9.9783e-01,  2.4715e-02, -2.0072e-01,  4.4556e-01,\n",
       "          1.4519e-01,  8.5913e-01, -9.3777e-01, -1.3680e-01, -9.8876e-01,\n",
       "          8.6671e-01, -9.0689e-01, -9.8297e-01, -3.7837e-01, -1.5410e-01,\n",
       "         -8.2690e-01,  9.9024e-01,  9.5905e-01,  9.9986e-01, -9.9953e-01,\n",
       "          7.0443e-01,  1.0577e-03,  9.9847e-01,  5.3667e-02, -6.1566e-01,\n",
       "          9.5620e-01,  9.9917e-01, -8.7763e-01,  9.3115e-01, -1.3026e-01,\n",
       "          3.3763e-02,  5.2268e-01, -6.4140e-01,  9.9912e-01, -9.3583e-01,\n",
       "          2.0355e-01, -9.7224e-01, -9.9974e-01,  9.9979e-01,  1.9186e-02,\n",
       "          9.8800e-01,  2.6715e-01,  8.7774e-01, -7.6033e-01,  9.9135e-01,\n",
       "         -9.9213e-01, -7.9592e-01, -9.9992e-01,  5.7811e-02, -8.9047e-01,\n",
       "         -9.8526e-01, -1.2248e-02,  9.7169e-01, -9.9904e-01, -9.8502e-01,\n",
       "         -4.2133e-01, -9.9993e-01,  9.7835e-01, -9.9376e-01, -9.1548e-01,\n",
       "         -9.7483e-01,  9.9914e-01, -2.7573e-02, -8.7118e-01,  9.4865e-01,\n",
       "         -9.5286e-01,  9.7533e-01,  9.3933e-01, -7.5245e-01,  1.7242e-01,\n",
       "         -3.0104e-02, -8.4035e-01, -9.9807e-01, -9.3658e-01, -9.6400e-01,\n",
       "          8.0781e-01, -9.8545e-01, -6.4387e-01,  9.9385e-01,  9.8420e-01,\n",
       "         -9.9830e-01, -9.9464e-01,  9.9827e-01,  5.4852e-02,  9.8453e-01,\n",
       "         -2.4473e-01, -9.9956e-01, -9.9980e-01,  6.9159e-02,  7.6273e-02,\n",
       "          9.9269e-01, -2.8464e-01,  7.2024e-01,  6.1729e-01, -2.3709e-01,\n",
       "          7.8155e-01, -7.2777e-01, -5.9031e-01, -6.4536e-01, -8.2757e-02,\n",
       "          9.9988e-01, -9.2857e-01,  9.8745e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.index(\"child\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    return bert_model(**encoded_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_2 = \"this is the painkiller\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = tokenizer.tokenize(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = predict(sentence)\n",
    "out2 = predict(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = out1[0:, tokens.index(\"this\"), :].detach()\n",
    "emb2 = out2[0:, tokens2.index(\"this\"), :].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1.shape, emb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7541961063436963"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(emb1.squeeze(), emb2.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "mask_model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = tokenizer.mask_token\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentence = f\"Wings of {mask} and wax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_tokens = tokenizer.tokenize(masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod_msk_imp = tokenizer(masked_sentence, return_tensors=\"pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.1960,  -7.0441,  -7.2008,  ...,  -5.9297,  -5.8665,  -6.3234],\n",
       "         [ -7.7212,  -7.9658,  -7.7128,  ...,  -6.3199,  -6.3678,  -6.8114],\n",
       "         [-10.1443, -10.0397,  -9.9595,  ...,  -8.2243,  -9.0468,  -8.8738],\n",
       "         ...,\n",
       "         [ -8.3468,  -7.9259,  -8.5168,  ...,  -6.5602,  -6.4394,  -6.8533],\n",
       "         [ -5.6904,  -5.6527,  -5.7449,  ...,  -4.0989,  -4.8940,  -5.3017],\n",
       "         [ -9.7534,  -9.4905,  -9.2125,  ...,  -8.0140,  -8.0205,  -9.9596]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = mask_model(**encod_msk_imp)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits.detach().numpy()[0]\n",
    "mask_logits = logits[msk_tokens.index(mask) + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.2519666e-09, 1.3354393e-08, 9.1484962e-09, ..., 2.5401373e-08,\n",
       "       1.5951899e-08, 8.1339708e-09], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_scores = softmax(mask_logits)\n",
    "confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wax 0.21704489\n",
      "Wings of wax and wax \n",
      "\n",
      "paper 0.078052744\n",
      "Wings of paper and wax \n",
      "\n",
      "glass 0.04618967\n",
      "Wings of glass and wax \n",
      "\n",
      "fire 0.044046085\n",
      "Wings of fire and wax \n",
      "\n",
      "silk 0.04300455\n",
      "Wings of silk and wax \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterating over the top 5 predicted tokens and printing the sentences with the masked token replaced\n",
    "for i in np.argsort(confidence_scores)[::-1][:5]:\n",
    "    pred_token = tokenizer.decode(i)\n",
    "    score = confidence_scores[i]\n",
    "\n",
    "    print(pred_token, score)\n",
    "    print(masked_sentence.replace(mask, pred_token), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
