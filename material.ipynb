{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source 1:** https://medium.com/analytics-vidhya/tokenization-building-a-tokenizer-and-a-sentencizer-c19a00393c19\n",
    "    - Coding a tokenizer and a sentencer\n",
    "- **Source 2:** https://medium.com/the-research-nest/explained-tokens-and-embeddings-in-llms-69a16ba5db33\n",
    "    - Tokenization methods\n",
    "    - Vectorization techniques & embeddings\n",
    "- **Source 3:** https://medium.com/@kashyapkathrani/all-about-embeddings-829c8ff0bf5b\n",
    "    - Word embeddings\n",
    "        - Word2Vec x GloVe x FastText\n",
    "    - Sentence embeddings\n",
    "        - ELMo x InferSent x Sentence-BERT\n",
    "- **Source 4:** https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac\n",
    "    - Word2Vec principles and coding\n",
    "- **Source 5:** http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    - Word2Vec principles (vanilla)\n",
    "- **Source 6:** http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "    - Word2Vec (Subsampling)\n",
    "    - Word2Vec (Negative sampling)\n",
    "- **Source 7:** https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
    "    - Position embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negative sampling\n",
    "- Hierarchical softmax\n",
    "- Positional embeddings\n",
    "- Contrastive learning\n",
    "- CNN\n",
    "    - Generating fixed-size outputs with CNN\n",
    "    - CNN + Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"Applying word2vec to Recommenders and Advertising\":** http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/\n",
    "- **\"The Transformer Attention Mechanism\":** https://machinelearningmastery.com/the-transformer-attention-mechanism/\n",
    "- **\"The Transformer Model\":** https://machinelearningmastery.com/the-transformer-model/\n",
    "- **\"Modelo de transformador para compreens√£o da linguagem \":** https://www.tensorflow.org/text/tutorials/transformer?hl=pt-br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more**\n",
    "    - https://www.amazon.com.br/Transformers-Natural-Language-Processing-architectures-ebook/dp/B08S977X8K/ref=sr_1_1?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2KJ0Z3P319SJA&dib=eyJ2IjoiMSJ9.szKdKYnC1krb773tF16jFgeBHBQlN_XlNUpTH-WGNbnFByzPiN1wewUCFMYjJRFfEWWKvhFbBmW-tFWJbrakHlFanETSPaEmQ9p2oByNGVvWa9TBRiVlkcH0IYLjxYLjInNjvTLV7MN4pPJ2LQLmibAQvwFcu5LO7e8Po-QmJO5zGrB9Dew9RTK-ONbdUpsBCv9mog0CQzIyzu3kZ6M0fnBEJSzo0PMYVdjXH6OR1fk.2uQeK7JozwkR8wcyv-2HQyg1r4kniRwyTI5WSqZoDGw&dib_tag=se&keywords=Transformers+for+Natural+Language+Processing%3A+Build+innovative+deep+neural+network+architectures+for+NLP+with+Python%2C+PyTorch%2C+TensorFlow%2C+BERT%2C+RoBERTa%2C+and+more&qid=1729910271&s=digital-text&sprefix=transformers+for+natural+language+processing+build+innovative+deep+neural+network+architectures+for+nlp+with+python+pytorch+tensorflow+bert+roberta+and+more+%2Cdigital-text%2C344&sr=1-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
